{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5XwvL5tjZqC"
      },
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Jelen Jupyter notebook a Budapesti Műszaki és Gazdaságtudományi Egyetemen tartott \"Deep Learning a gyakorlatban Python és LUA alapon\" tantárgy segédanyagaként készült.\n",
        "A tantárgy honlapja: http://smartlab.tmit.bme.hu/oktatas-deep-learning\n",
        "Deep Learning kutatás: http://smartlab.tmit.bme.hu/deep-learning\n",
        "Jelen notebook a hivatalos dokumentáció alapján készült: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\n",
        "\n",
        "A notebook bármely részének újra felhasználása, publikálása csak a szerzők írásos beleegyezése esetén megengedett.\n",
        "\n",
        "2019 (c) Császár Márk, Gyires-Tóth Bálint (toth.b kukac tmit pont bme pont hu)\n",
        "</PRE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXsWcB3gkqye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an50QDyTohta",
        "outputId": "336c05e0-7d90-4a0e-b755-9099466063df"
      },
      "outputs": [],
      "source": [
        "# download train and test splits\n",
        "train_dataset = datasets.CIFAR10(root=\"datasets\", train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.CIFAR10(root=\"datasets\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# random split training dataset to train and validation splits\n",
        "train_dataset, val_dataset = random_split(train_dataset, [45000, 5000])\n",
        "\n",
        "# create batched dataloaders\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwRhnZevpumZ"
      },
      "outputs": [],
      "source": [
        "# Task 1\n",
        "# Create a file called \"solution_1.txt\"\n",
        "# Write your Neptun code to line 1\n",
        "\n",
        "# Task 2\n",
        "# Create a file called \"solution_2.txt\"\n",
        "# Write the train dataset size to line 1\n",
        "# Write the validation dataset size to line 2\n",
        "# Write the test dataset size to the line 3\n",
        "# Write the train dataloader length to line 4\n",
        "# Write the validation dataloader length to line 5\n",
        "# Write the test dataloader length to line 6\n",
        "\n",
        "# Task 3\n",
        "# Create a file called \"solution_3.txt\"\n",
        "# Write the shape of the first training batch to line 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
